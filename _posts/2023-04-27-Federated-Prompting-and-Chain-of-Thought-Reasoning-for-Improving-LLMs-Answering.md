---
layout: post
author: [Xiangyang Liu,Tianqi Pang,Chenyou Fan]
title: Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering
---
We investigate how to enhance answer precision in frequently asked questions posed by distributed users using cloud-based Large Language Models (LLMs). Our study focuses on a typical situations where users ask similar queries that involve identical mathematical reasoning steps and problem-solving procedures. Due to the unsatisfactory accuracy of LLMs' zero-shot prompting with standalone questions, we propose to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques. Specifically, we first retrieve synonymous questions from a crowd-sourced database and create a federated question pool. We call these federated synonymous questions with the same or different parameters SP-questions or DP-questions, respectively. We refer to our methods as Fed-SP-SC and Fed-DP-CoT, which can generate significantly more accurate answers for all user queries without requiring sophisticated model-tuning. Through extensive experiments, we demonstrate that our proposed methods can significantly enhance question accuracy by fully exploring the synonymous nature of the questions and the consistency of the answers.

###### International Conference on Knowledge Science, Engineering and Management (KSEM-23)

<a href="https://arxiv.org/abs/2304.13911#:~:text=Federated%20Prompting%20and%20Chain-of-Thought%20Reasoning%20for%20Improving%20LLMs,distributed%20users%20using%20cloud-based%20Large%20Language%20Models%20%28LLMs%29." target="_blank">for more information</a>
