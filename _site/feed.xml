<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-07-03T14:18:37+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Chenyou Fan</title><subtitle>Associate Professor of School of Artificial Intelligence, South China Normal University.</subtitle><author><name>Chenyou Fan (Ph.D.)</name></author><entry><title type="html">Carbon Price Forecasting with Quantile Regression and Feature Selection</title><link href="http://localhost:4000/2023/05/05/Carbon-Price-Forecasting-with-Quantile-Regression-and-Feature-Selection.html" rel="alternate" type="text/html" title="Carbon Price Forecasting with Quantile Regression and Feature Selection" /><published>2023-05-05T00:00:00+08:00</published><updated>2023-05-05T00:00:00+08:00</updated><id>http://localhost:4000/2023/05/05/Carbon-Price-Forecasting-with-Quantile-Regression-and-Feature-Selection</id><content type="html" xml:base="http://localhost:4000/2023/05/05/Carbon-Price-Forecasting-with-Quantile-Regression-and-Feature-Selection.html"><![CDATA[<p>Carbon futures has recently emerged as a novel financial asset in the trading markets such as the European Union and China. Monitoring the trend of the carbon price has become critical for both national policy-making as well as industrial manufacturing planning. However, various geopolitical, social, and economic factors can impose substantial influence on the carbon price. Due to its volatility and non-linearity, predicting accurate carbon prices is generally a difficult task. In this study, we propose to improve carbon price forecasting with several novel practices. First, we collect various influencing factors, including commodity prices, export volumes such as oil and natural gas, and prosperity indices. Then we select the most significant factors and disclose their optimal grouping for explainability. Finally, we use the Sparse Quantile Group Lasso and Adaptive Sparse Quantile Group Lasso for robust price predictions. We demonstrate through extensive experimental studies that our proposed methods outperform existing ones. Also, our quantile predictions provide a complete profile of future prices at different levels, which better describes the distributions of the carbon market.</p>

<h6 id="international-conference-on-data-mining-and-knowledge-discovery-dmkd-23">International Conference on Data Mining and Knowledge Discovery (DMKD-23)</h6>

<p><a href="https://arxiv.org/abs/2305.03224" target="_blank">for more information</a></p>]]></content><author><name>[&quot;Tianqi Pang&quot;, &quot;Chenyou Fan&quot;]</name></author><summary type="html"><![CDATA[Carbon futures has recently emerged as a novel financial asset in the trading markets such as the European Union and China. Monitoring the trend of the carbon price has become critical for both national policy-making as well as industrial manufacturing planning. However, various geopolitical, social, and economic factors can impose substantial influence on the carbon price. Due to its volatility and non-linearity, predicting accurate carbon prices is generally a difficult task. In this study, we propose to improve carbon price forecasting with several novel practices. First, we collect various influencing factors, including commodity prices, export volumes such as oil and natural gas, and prosperity indices. Then we select the most significant factors and disclose their optimal grouping for explainability. Finally, we use the Sparse Quantile Group Lasso and Adaptive Sparse Quantile Group Lasso for robust price predictions. We demonstrate through extensive experimental studies that our proposed methods outperform existing ones. Also, our quantile predictions provide a complete profile of future prices at different levels, which better describes the distributions of the carbon market.]]></summary></entry><entry><title type="html">Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering</title><link href="http://localhost:4000/2023/04/27/Federated-Prompting-and-Chain-of-Thought-Reasoning-for-Improving-LLMs-Answering.html" rel="alternate" type="text/html" title="Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering" /><published>2023-04-27T00:00:00+08:00</published><updated>2023-04-27T00:00:00+08:00</updated><id>http://localhost:4000/2023/04/27/Federated-Prompting-and-Chain-of-Thought-Reasoning-for-Improving-LLMs-Answering</id><content type="html" xml:base="http://localhost:4000/2023/04/27/Federated-Prompting-and-Chain-of-Thought-Reasoning-for-Improving-LLMs-Answering.html"><![CDATA[<p>We investigate how to enhance answer precision in frequently asked questions posed by distributed users using cloud-based Large Language Models (LLMs). Our study focuses on a typical situations where users ask similar queries that involve identical mathematical reasoning steps and problem-solving procedures. Due to the unsatisfactory accuracy of LLMs’ zero-shot prompting with standalone questions, we propose to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques. Specifically, we first retrieve synonymous questions from a crowd-sourced database and create a federated question pool. We call these federated synonymous questions with the same or different parameters SP-questions or DP-questions, respectively. We refer to our methods as Fed-SP-SC and Fed-DP-CoT, which can generate significantly more accurate answers for all user queries without requiring sophisticated model-tuning. Through extensive experiments, we demonstrate that our proposed methods can significantly enhance question accuracy by fully exploring the synonymous nature of the questions and the consistency of the answers.</p>

<h6 id="international-conference-on-knowledge-science-engineering-and-management-ksem-23">International Conference on Knowledge Science, Engineering and Management (KSEM-23)</h6>

<p><a href="https://arxiv.org/abs/2304.13911#:~:text=Federated%20Prompting%20and%20Chain-of-Thought%20Reasoning%20for%20Improving%20LLMs,distributed%20users%20using%20cloud-based%20Large%20Language%20Models%20%28LLMs%29." target="_blank">for more information</a></p>]]></content><author><name>[&quot;Xiangyang Liu&quot;, &quot;Tianqi Pang&quot;, &quot;Chenyou Fan&quot;]</name></author><summary type="html"><![CDATA[We investigate how to enhance answer precision in frequently asked questions posed by distributed users using cloud-based Large Language Models (LLMs). Our study focuses on a typical situations where users ask similar queries that involve identical mathematical reasoning steps and problem-solving procedures. Due to the unsatisfactory accuracy of LLMs’ zero-shot prompting with standalone questions, we propose to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques. Specifically, we first retrieve synonymous questions from a crowd-sourced database and create a federated question pool. We call these federated synonymous questions with the same or different parameters SP-questions or DP-questions, respectively. We refer to our methods as Fed-SP-SC and Fed-DP-CoT, which can generate significantly more accurate answers for all user queries without requiring sophisticated model-tuning. Through extensive experiments, we demonstrate that our proposed methods can significantly enhance question accuracy by fully exploring the synonymous nature of the questions and the consistency of the answers.]]></summary></entry><entry><title type="html">Where to Attack：A Dynamic Locator Model for Backdoor Attack in Text Classifications</title><link href="http://localhost:4000/2022/10/18/where-to-attack-a-dynamic-locator-model-for-backdoor-attack-in-text-classifications.html" rel="alternate" type="text/html" title="Where to Attack：A Dynamic Locator Model for Backdoor Attack in Text Classifications" /><published>2022-10-18T00:00:00+08:00</published><updated>2022-10-18T00:00:00+08:00</updated><id>http://localhost:4000/2022/10/18/where-to-attack-a-dynamic-locator-model-for-backdoor-attack-in-text-classifications</id><content type="html" xml:base="http://localhost:4000/2022/10/18/where-to-attack-a-dynamic-locator-model-for-backdoor-attack-in-text-classifications.html"><![CDATA[<p>Nowadays, deep-learning based NLP models are usually trained with large-scale third-party data which can be easily injected with malicious backdoors. Thus, BackDoor Attack (BDA) study has become a trending research to help promote the robustness of an NLP system. Text-based BDA aims to train a poisoned model with both clean and poisoned texts to perform normally on clean inputs while being misled to predict those trigger-embedded texts as target labels set by attackers. Previous works usually choose fixed Positions-to-Poison (P2P) first, then add triggers upon those positions such as letter insertion or deletion. However, considering the positions of words with important semantics may vary in different contexts, fixed P2P models are severely limited in flexibility and performance. We study the text-based BDA from the perspective of automatically and dynamically selecting P2P from contexts. We design a novel Locator model which can predict P2P dynamically without human intervention. Based on the predicted P2P, four effective strategies are introduced to show the BDA performance. Experiments on two public datasets show both tinier test accuracy gap on clean data and higher attack success rate on poisoned ones. Human evaluation with volunteers also shows the P2P predicted by our model are important for classification.</p>

<h6 id="international-conference-on-computational-linguistics-coling-22">International Conference on Computational Linguistics (COLING-22)</h6>

<p><a href="https://github.com/jncsnlp/LocatorModel" target="_blank">Source code is available here</a><br />
<a href="https://arxiv.org/abs/2305.03224" target="_blank">for more information</a></p>]]></content><author><name>[&quot;Heng-yang Lu&quot;, &quot;Chenyou Fan&quot;, &quot;Jun Yang&quot;, &quot;Cong Hu&quot;, &quot;Wei Fang&quot;, &quot;Xiao-jun Wu&quot;]</name></author><summary type="html"><![CDATA[Nowadays, deep-learning based NLP models are usually trained with large-scale third-party data which can be easily injected with malicious backdoors. Thus, BackDoor Attack (BDA) study has become a trending research to help promote the robustness of an NLP system. Text-based BDA aims to train a poisoned model with both clean and poisoned texts to perform normally on clean inputs while being misled to predict those trigger-embedded texts as target labels set by attackers. Previous works usually choose fixed Positions-to-Poison (P2P) first, then add triggers upon those positions such as letter insertion or deletion. However, considering the positions of words with important semantics may vary in different contexts, fixed P2P models are severely limited in flexibility and performance. We study the text-based BDA from the perspective of automatically and dynamically selecting P2P from contexts. We design a novel Locator model which can predict P2P dynamically without human intervention. Based on the predicted P2P, four effective strategies are introduced to show the BDA performance. Experiments on two public datasets show both tinier test accuracy gap on clean data and higher attack success rate on poisoned ones. Human evaluation with volunteers also shows the P2P predicted by our model are important for classification.]]></summary></entry><entry><title type="html">Private Semi-Supervised Federated Learning</title><link href="http://localhost:4000/2022/07/18/Private-Semi-Supervised-Federated-Learning.html" rel="alternate" type="text/html" title="Private Semi-Supervised Federated Learning" /><published>2022-07-18T00:00:00+08:00</published><updated>2022-07-18T00:00:00+08:00</updated><id>http://localhost:4000/2022/07/18/Private-Semi-Supervised-Federated-Learning</id><content type="html" xml:base="http://localhost:4000/2022/07/18/Private-Semi-Supervised-Federated-Learning.html"><![CDATA[<p>We study a federated learning (FL) framework to effectively train models from scarce and skewly distributed labeled data. We consider a challenging yet practical scenario: a few data sources own a small amount of labeled data, while the rest mass sources own purely unlabeled data. Classical FL requires each client to have enough labeled data for local training, thus is not applicable in this scenario. In this work, we design an effective federated semi-supervised learning framework (FedSSL) to fully leverage both labeled and unlabeled data sources. We establish a unified data space across all participating agents, so that each agent can generate mixed data samples to boost semi-supervised learning (SSL), while keeping data locality. We further show that FedSSL can integrate differential privacy protection techniques to prevent labeled data leakage at the cost of minimum performance degradation. On SSL tasks with as small as 0.17% and 1% of MNIST and CIFAR-10 datasets as labeled data, respectively, our approach can achieve 5-20% performance boost over the state-of-the-art methods.</p>

<h6 id="31st-international-joint-conference-on-artificial-intelligence-ijcai22-15-acceptance-rate">31st International Joint Conference on Artificial Intelligence (IJCAI’22, 15% acceptance rate)</h6>

<p><a href="https://www.ijcai.org/proceedings/2022/279" target="_blank">for more information</a></p>]]></content><author><name>[&quot;Chenyou Fan&quot;, &quot;Junjie Hu&quot;, &quot;Jianwei Huang&quot;]</name></author><summary type="html"><![CDATA[We study a federated learning (FL) framework to effectively train models from scarce and skewly distributed labeled data. We consider a challenging yet practical scenario: a few data sources own a small amount of labeled data, while the rest mass sources own purely unlabeled data. Classical FL requires each client to have enough labeled data for local training, thus is not applicable in this scenario. In this work, we design an effective federated semi-supervised learning framework (FedSSL) to fully leverage both labeled and unlabeled data sources. We establish a unified data space across all participating agents, so that each agent can generate mixed data samples to boost semi-supervised learning (SSL), while keeping data locality. We further show that FedSSL can integrate differential privacy protection techniques to prevent labeled data leakage at the cost of minimum performance degradation. On SSL tasks with as small as 0.17% and 1% of MNIST and CIFAR-10 datasets as labeled data, respectively, our approach can achieve 5-20% performance boost over the state-of-the-art methods.]]></summary></entry><entry><title type="html">Few-Shot Multi-Agent Perception</title><link href="http://localhost:4000/2021/10/17/Few-Shot-Multi-Agent-Perception.html" rel="alternate" type="text/html" title="Few-Shot Multi-Agent Perception" /><published>2021-10-17T00:00:00+08:00</published><updated>2021-10-17T00:00:00+08:00</updated><id>http://localhost:4000/2021/10/17/Few-Shot-Multi-Agent-Perception</id><content type="html" xml:base="http://localhost:4000/2021/10/17/Few-Shot-Multi-Agent-Perception.html"><![CDATA[<p>We study few-shot learning (FSL) under multi-agent scenarios, in which participating agents only have local scarce labeled data and need to collaborate to predict query data labels. Though each of the agents, such as drones and robots, has minimal communication and computation capability, we aim at designing coordination schemes such that they can collectively perceive the environment accurately and efficiently. We propose a novel metric-based multi-agent FSL framework which has three main components: an efficient communication mechanism that propagates compact and fine-grained query feature maps from query agents to support agents; an asymmetric attention mechanism that computes region-level attention weights between query and support feature maps; and a metric-learning module which calculates the image-level relevance between query and support data fast and accurately. Through analysis and extensive numerical studies, we demonstrate that our approach can save communication and computation costs and significantly improve performance in both visual and acoustic perception tasks such as face identification, semantic segmentation, and sound genre recognition.</p>

<h6 id="29th-acm-international-conference-on-multimedia-2021-acm-mm21-279-acceptance-rate">29th ACM International Conference on Multimedia 2021 (ACM MM’21, 27.9% acceptance rate)</h6>

<p><a href="https://dl.acm.org/doi/10.1145/3474085.3475315" target="_blank">for more information</a></p>]]></content><author><name>[&quot;Chenyou Fan&quot;, &quot;Junjie Hu&quot;, &quot;Jianwei Huang&quot;]</name></author><summary type="html"><![CDATA[We study few-shot learning (FSL) under multi-agent scenarios, in which participating agents only have local scarce labeled data and need to collaborate to predict query data labels. Though each of the agents, such as drones and robots, has minimal communication and computation capability, we aim at designing coordination schemes such that they can collectively perceive the environment accurately and efficiently. We propose a novel metric-based multi-agent FSL framework which has three main components: an efficient communication mechanism that propagates compact and fine-grained query feature maps from query agents to support agents; an asymmetric attention mechanism that computes region-level attention weights between query and support feature maps; and a metric-learning module which calculates the image-level relevance between query and support data fast and accurately. Through analysis and extensive numerical studies, we demonstrate that our approach can save communication and computation costs and significantly improve performance in both visual and acoustic perception tasks such as face identification, semantic segmentation, and sound genre recognition.]]></summary></entry><entry><title type="html">Projection Robust Wasserstein Distance and Riemannian Optimization</title><link href="http://localhost:4000/2020/06/12/Projection-Robust-Wasserstein-Distance-and-Riemannian-Optimization.html" rel="alternate" type="text/html" title="Projection Robust Wasserstein Distance and Riemannian Optimization" /><published>2020-06-12T00:00:00+08:00</published><updated>2020-06-12T00:00:00+08:00</updated><id>http://localhost:4000/2020/06/12/Projection-Robust-Wasserstein-Distance-and-Riemannian-Optimization</id><content type="html" xml:base="http://localhost:4000/2020/06/12/Projection-Robust-Wasserstein-Distance-and-Riemannian-Optimization.html"><![CDATA[<p>Projection robust Wasserstein (PRW) distance, or Wasserstein projection pursuit (WPP), is a robust variant of the Wasserstein distance. Recent work suggests that this quantity is more robust than the standard Wasserstein distance, in particular when comparing probability measures in high-dimensions. However, it is ruled out for practical application because the optimization model is essentially non-convex and non-smooth which makes the computation intractable. Our contribution in this paper is to revisit the original motivation behind WPP/PRW, but take the hard route of showing that, despite its non-convexity and lack of nonsmoothness, and even despite some hardness results proved by~\citet{Niles-2019-Estimation} in a minimax sense, the original formulation for PRW/WPP \textit{can} be efficiently computed in practice using Riemannian optimization, yielding in relevant cases better behavior than its convex relaxation. More specifically, we provide three simple algorithms with solid theoretical guarantee on their complexity bound (one in the appendix), and demonstrate their effectiveness and efficiency by conducing extensive experiments on synthetic and real data. This paper provides a first step into a computational theory of the PRW distance and provides the links between optimal transport and Riemannian optimization.</p>

<h6 id="conference-on-neural-information-processing-systems-neurips20-spotlight-20-acceptance-rate-3-spotlight-rate">Conference on Neural Information Processing Systems (NeurIPS’20, spotlight, 20% acceptance rate, 3% spotlight rate)</h6>

<p><a href="https://arxiv.org/abs/2006.07458" target="_blank">for more information</a></p>]]></content><author><name>[&quot;Tianyi Lin&quot;, &quot;Chenyou Fan&quot;, &quot;Nhat Ho&quot;, &quot;Marco Cuturi&quot;, &quot;Michael I. Jordan&quot;]</name></author><summary type="html"><![CDATA[Projection robust Wasserstein (PRW) distance, or Wasserstein projection pursuit (WPP), is a robust variant of the Wasserstein distance. Recent work suggests that this quantity is more robust than the standard Wasserstein distance, in particular when comparing probability measures in high-dimensions. However, it is ruled out for practical application because the optimization model is essentially non-convex and non-smooth which makes the computation intractable. Our contribution in this paper is to revisit the original motivation behind WPP/PRW, but take the hard route of showing that, despite its non-convexity and lack of nonsmoothness, and even despite some hardness results proved by~\citet{Niles-2019-Estimation} in a minimax sense, the original formulation for PRW/WPP \textit{can} be efficiently computed in practice using Riemannian optimization, yielding in relevant cases better behavior than its convex relaxation. More specifically, we provide three simple algorithms with solid theoretical guarantee on their complexity bound (one in the appendix), and demonstrate their effectiveness and efficiency by conducing extensive experiments on synthetic and real data. This paper provides a first step into a computational theory of the PRW distance and provides the links between optimal transport and Riemannian optimization.]]></summary></entry><entry><title type="html">Multi-Horizon Time Series Forecasting with Temporal Attention Learning</title><link href="http://localhost:4000/2019/07/25/Multi-Horizon-Time-Series-Forecasting-with-Temporal-Attention-Learning.html" rel="alternate" type="text/html" title="Multi-Horizon Time Series Forecasting with Temporal Attention Learning" /><published>2019-07-25T00:00:00+08:00</published><updated>2019-07-25T00:00:00+08:00</updated><id>http://localhost:4000/2019/07/25/Multi-Horizon-Time-Series-Forecasting-with-Temporal-Attention-Learning</id><content type="html" xml:base="http://localhost:4000/2019/07/25/Multi-Horizon-Time-Series-Forecasting-with-Temporal-Attention-Learning.html"><![CDATA[<p>We propose a novel data-driven approach for solving multi-horizon probabilistic forecasting tasks that predicts the full distribution of a time series on future horizons. We illustrate that temporal patterns hidden in historical information play an important role in accurate forecasting of long time series. Traditional methods rely on setting up temporal dependencies manually to explore related patterns in historical data, which is unrealistic in forecasting long-term series on real-world data. Instead, we propose to explicitly learn constructing hidden patterns’ representations with deep neural networks and attending to different parts of the history for forecasting the future.</p>

<p>In this paper, we propose an end-to-end deep-learning framework for multi-horizon time series forecasting, with temporal attention mechanisms to better capture latent patterns in historical data which are useful in predicting the future. Forecasts of multiple quantiles on multiple future horizons can be generated simultaneously based on the learned latent pattern features. We also propose a multimodal fusion mechanism which is used to combine features from different parts of the history to better represent the future. Experiment results demonstrate our approach achieves state-of-the-art performance on two large-scale forecasting datasets in different domains.</p>

<h6 id="sigkdd-conference-on-knowledge-discovery-and-data-mining-2019-kdd19-20-acceptance-rate">SIGKDD Conference on Knowledge Discovery and Data Mining 2019 (KDD’19, 20% acceptance rate)</h6>

<p><a href="https://dl.acm.org/doi/10.1145/3292500.3330662" target="_blank">for more information</a><br /></p>]]></content><author><name>[&quot;Chenyou Fan&quot;, &quot;Yuze Zhang&quot;, &quot;Yi Pan&quot;, &quot;Xiaoyue Li&quot;, &quot;Chi Zhang&quot;, &quot;Rong Yuan&quot;, &quot;Di Wu&quot;, &quot;Wensheng Wang&quot;, &quot;Jian Pei&quot;, &quot;Heng Huang&quot;]</name></author><summary type="html"><![CDATA[We propose a novel data-driven approach for solving multi-horizon probabilistic forecasting tasks that predicts the full distribution of a time series on future horizons. We illustrate that temporal patterns hidden in historical information play an important role in accurate forecasting of long time series. Traditional methods rely on setting up temporal dependencies manually to explore related patterns in historical data, which is unrealistic in forecasting long-term series on real-world data. Instead, we propose to explicitly learn constructing hidden patterns’ representations with deep neural networks and attending to different parts of the history for forecasting the future.]]></summary></entry><entry><title type="html">Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering</title><link href="http://localhost:4000/2019/04/08/Heterogeneous-Memory-Enhanced-Multimodal-Attention-Model-for-Video-Question-Answering.html" rel="alternate" type="text/html" title="Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering" /><published>2019-04-08T00:00:00+08:00</published><updated>2019-04-08T00:00:00+08:00</updated><id>http://localhost:4000/2019/04/08/Heterogeneous-Memory-Enhanced-Multimodal-Attention-Model-for-Video-Question-Answering</id><content type="html" xml:base="http://localhost:4000/2019/04/08/Heterogeneous-Memory-Enhanced-Multimodal-Attention-Model-for-Video-Question-Answering.html"><![CDATA[<p>In this paper, we propose a novel end-to-end trainable Video Question Answering (VideoQA) framework with three major components: 1) a new heterogeneous memory which can effectively learn global context information from appearance and motion features; 2) a redesigned question memory which helps understand the complex semantics of question and highlights queried subjects; and 3) a new multimodal fusion layer which performs multi-step reasoning by attending to relevant visual and textual hints with self-updated attention. Our VideoQA model firstly generates the global context-aware visual and textual features respectively by interacting current inputs with memory contents. After that, it makes the attentional fusion of the multimodal visual and textual representations to infer the correct answer. Multiple cycles of reasoning can be made to iteratively refine attention weights of the multimodal data and improve the final representation of the QA pair. Experimental results demonstrate our approach achieves state-of-the-art performance on four VideoQA benchmark datasets.</p>

<h6 id="ieee-conference-on-computer-vision-and-pattern-recognition-2019-cvpr19-252-acceptance-rate">IEEE Conference on Computer Vision and Pattern Recognition 2019 (CVPR’19, 25.2% acceptance rate)</h6>

<p><a href="https://arxiv.org/abs/1904.04357" target="_blank">for more information</a><br /></p>]]></content><author><name>[&quot;Chenyou Fan&quot;, &quot;Xiaofan Zhang&quot;, &quot;Shu Zhang&quot;, &quot;Wensheng Wang&quot;, &quot;Chi Zhang&quot;, &quot;Heng Huang&quot;]</name></author><summary type="html"><![CDATA[In this paper, we propose a novel end-to-end trainable Video Question Answering (VideoQA) framework with three major components: 1) a new heterogeneous memory which can effectively learn global context information from appearance and motion features; 2) a redesigned question memory which helps understand the complex semantics of question and highlights queried subjects; and 3) a new multimodal fusion layer which performs multi-step reasoning by attending to relevant visual and textual hints with self-updated attention. Our VideoQA model firstly generates the global context-aware visual and textual features respectively by interacting current inputs with memory contents. After that, it makes the attentional fusion of the multimodal visual and textual representations to infer the correct answer. Multiple cycles of reasoning can be made to iteratively refine attention weights of the multimodal data and improve the final representation of the QA pair. Experimental results demonstrate our approach achieves state-of-the-art performance on four VideoQA benchmark datasets.]]></summary></entry><entry><title type="html">Improved Sample Complexity for Stochastic Compositional Variance Reduced Gradient</title><link href="http://localhost:4000/2018/06/01/Improved-Sample-Complexity-for-Stochastic-Compositional-Variance-Reduced-Gradient.html" rel="alternate" type="text/html" title="Improved Sample Complexity for Stochastic Compositional Variance Reduced Gradient" /><published>2018-06-01T00:00:00+08:00</published><updated>2018-06-01T00:00:00+08:00</updated><id>http://localhost:4000/2018/06/01/Improved-Sample-Complexity-for-Stochastic-Compositional-Variance-Reduced-Gradient</id><content type="html" xml:base="http://localhost:4000/2018/06/01/Improved-Sample-Complexity-for-Stochastic-Compositional-Variance-Reduced-Gradient.html"><![CDATA[<p>Convex composition optimization is an emerging topic that covers a wide range of applications arising from stochastic optimal control, reinforcement learning and multi-stage stochastic programming. Existing algorithms suffer from unsatisfactory sample complexity and practical issues since they ignore the convexity structure in the algorithmic design. In this paper, we develop a new stochastic compositional variance-reduced gradient algorithm with the sample complexity of <em>O((m+n)log(1/ϵ)+1/ϵ^3)</em> where <em>m+n</em> is the total number of samples. Our algorithm is near-optimal as the dependence on m+n is optimal up to a logarithmic factor. Experimental results on real-world datasets demonstrate the effectiveness and efficiency of the new algorithm.</p>

<h6 id="american-control-conference-2020-acc20">American Control Conference 2020 (ACC’20)</h6>

<p><a href="https://arxiv.org/abs/2006.07458" target="_blank">for more information</a><br /></p>]]></content><author><name>[&quot;Tianyi Lin&quot;, &quot;Chenyou Fan&quot;, &quot;Mengdi Wang&quot;, &quot;Michael I. Jordan&quot;]</name></author><summary type="html"><![CDATA[Convex composition optimization is an emerging topic that covers a wide range of applications arising from stochastic optimal control, reinforcement learning and multi-stage stochastic programming. Existing algorithms suffer from unsatisfactory sample complexity and practical issues since they ignore the convexity structure in the algorithmic design. In this paper, we develop a new stochastic compositional variance-reduced gradient algorithm with the sample complexity of O((m+n)log(1/ϵ)+1/ϵ^3) where m+n is the total number of samples. Our algorithm is near-optimal as the dependence on m+n is optimal up to a logarithmic factor. Experimental results on real-world datasets demonstrate the effectiveness and efficiency of the new algorithm.]]></summary></entry><entry><title type="html">Deepdiary： Lifelogging image captioning and summarization</title><link href="http://localhost:4000/2018/05/08/Deepdiary-Lifelogging-Image-Captioning-and-Summarization.html" rel="alternate" type="text/html" title="Deepdiary： Lifelogging image captioning and summarization" /><published>2018-05-08T00:00:00+08:00</published><updated>2018-05-08T00:00:00+08:00</updated><id>http://localhost:4000/2018/05/08/Deepdiary-Lifelogging-Image-Captioning-and-Summarization</id><content type="html" xml:base="http://localhost:4000/2018/05/08/Deepdiary-Lifelogging-Image-Captioning-and-Summarization.html"><![CDATA[<p>Automatic image captioning has been studied extensively over the last few years, driven by breakthroughs in deep learning-based image-to-text translation models. However, most of this work has considered captioning web images from standard data sets like MS-COCO, and has considered single images in isolation. To what extent can automatic captioning models learn finer-grained contextual information specific to a given person’s day-to-day visual experiences? In this paper, we consider captioning image sequences collected from wearable, life-logging cameras. Automatically-generated captions could help people find and recall photos among their large-scale life-logging photo collections, or even to produce textual “diaries” that summarize their day. But unlike web images, photos from wearable cameras are often blurry and poorly composed, without an obvious single subject. Their content also tends to be highly dependent on the context and characteristics of the particular camera wearer. To address these challenges, we introduce a technique to jointly caption sequences of photos, which allows captions to take advantage of temporal constraints and evidence across time, and we introduce a technique to increase the diversity of generated captions, so that they can describe a photo from multiple perspectives (e.g., first-person versus third-person). To test these techniques, we collect a dataset of about 8000 realistic lifelogging images, a subset of which are annotated with nearly 5000 human-generated reference sentences. We evaluate the quality of image captions both quantitatively and qualitatively using Amazon Mechanical Turk, finding that while these algorithms are not perfect, they could be an important step towards helping to organize and summarize lifelogging photos.</p>

<h6 id="2018-journal-of-visual-communication-and-image-representation">2018. Journal of Visual Communication and Image Representation</h6>

<p><a href="https://www.sciencedirect.com/science/article/abs/pii/S1047320318301032" target="_blank">for more information</a><br /></p>]]></content><author><name>[&quot;Chenyou Fan&quot;, &quot;Zehua Zhang&quot;, &quot;David J. Crandall&quot;]</name></author><summary type="html"><![CDATA[Automatic image captioning has been studied extensively over the last few years, driven by breakthroughs in deep learning-based image-to-text translation models. However, most of this work has considered captioning web images from standard data sets like MS-COCO, and has considered single images in isolation. To what extent can automatic captioning models learn finer-grained contextual information specific to a given person’s day-to-day visual experiences? In this paper, we consider captioning image sequences collected from wearable, life-logging cameras. Automatically-generated captions could help people find and recall photos among their large-scale life-logging photo collections, or even to produce textual “diaries” that summarize their day. But unlike web images, photos from wearable cameras are often blurry and poorly composed, without an obvious single subject. Their content also tends to be highly dependent on the context and characteristics of the particular camera wearer. To address these challenges, we introduce a technique to jointly caption sequences of photos, which allows captions to take advantage of temporal constraints and evidence across time, and we introduce a technique to increase the diversity of generated captions, so that they can describe a photo from multiple perspectives (e.g., first-person versus third-person). To test these techniques, we collect a dataset of about 8000 realistic lifelogging images, a subset of which are annotated with nearly 5000 human-generated reference sentences. We evaluate the quality of image captions both quantitatively and qualitatively using Amazon Mechanical Turk, finding that while these algorithms are not perfect, they could be an important step towards helping to organize and summarize lifelogging photos.]]></summary></entry></feed>